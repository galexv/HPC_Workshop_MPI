<[autotemplate]
title={MPI profiling with Allinea MAP}
author={Alexander Gaenko}
usetheme={Frankfurt}
usecolortheme={whale}
lstloadlanguages={sh,C++}
definecolor={babyblue}{rgb}{0.54, 0.81, 0.94}
definecolor={ballblue}{rgb}{0.13, 0.67, 0.8}
definecolor={otherblue}{RGB}{120,100,255}
lstset={escapeinside={\#(}{)},style=basic,emphstyle=\color{otherblue},basicstyle=\small\ttfamily,basewidth=0.5em}
[autotemplate]>

==== Problem: Calculation of a definite integral.  ====

<[columns][T]
[[[0.25\textwidth]]]
<<<integrand-labelled.pdf,height=0.9\textheight>>>
[[[0.75\textwidth]]]
The problem:
* We need to compute: \\ $ \displaystyle F(a,b) = \int_a^b f(x) dx $ \\ Where $ f(x) $ is some (presumably\\ ``slow to compute'') function.\\[0.8\baselineskip]
* We use this approach:
*# Split $[a,b]$ by points {$\{x_{1}, x_{2}, \ldots, x_{k}\}$}
*# $ \displaystyle F(a,b) = \sum_{k} f(\frac{x_{k}+x_{k+1}}{2}) (x_{k+1}-x_{k}) $
* The integrand $f(x)$ and the integration routine are hidden inside a library.

[columns]>

==== Sequential program code ====
<[code][language=C++,basicstyle=\tiny\ttfamily]
#include <stdio.h>
#include <math.h>

// Declare integrand() and integral() from ``mymath`` library
#include "mylib/mymath.hpp"

int main(int argc, char** argv)
{
  unsigned long int n;
  if (argc!=2 || sscanf(argv\[1\],"%lu",&n)!=1) {
    fprintf(stderr,"Usage:\n%s integration_steps\n\n\n",argv\[0\]);
    return 1;
  }

  // Integration limits.
  const double global_a=1E-5;
  const double global_b=1;

  // Perform integration
  const double y=integral(integrand, n, global_a, global_b);

  const double y_exact=4*(pow(global_b,0.25)-pow(global_a,0.25));
  printf("Result=%lf Exact=%lf Difference=%lf\n", y, y_exact, y-y_exact);

  return 0;
}
[code]>

==== Problem size behavior: how to measure. ====
Do we even need to parallelize? 
<[code][language=sh,emph={time}]
# Compilation:
$ gcc -O3 -o integral_seq.x integral_seq.cxx\
      -L./mylib -lmymath
# Timed runs:
$ time -p ./integral_seq.x #(\textcolor{blue}{1000000})
Result=3.775045 Exact=3.775063 Difference=-0.000019
real #(\textcolor{blue}{2.12})
$ time -p ./integral_seq.x #(\textcolor{blue}{2000000})
Result=3.775058 Exact=3.775063 Difference=-0.000005
real #(\textcolor{blue}{4.28})
$ time -p ./integral_seq.x #(\textcolor{blue}{8000000})
Result=3.775062 Exact=3.775063 Difference=-0.000001
real #(\textcolor{blue}{17.61})
[code]>
-- -0.5\baselineskip --
The time grows linearly with the problems size.
Acceptable accuracy at 8M points.
Can we speed it up?

==== Parallellization: domain decomposition. ====

Approach: 
* Split $[a,b]$ into several domains;
* Compute integrals _blue_independently_.
<[columns]
[[[0.4\textwidth]]]
<<<integrand-edited.pdf,height=0.5\textheight>>>
[[[0.6\textwidth]]]
\hspace{2em}$ \displaystyle F(a,b) = \int_a^b f(x) dx $
# Assign a process to each \\ domain $[x_{k}, x_{k+1}]$
# Let each process \\ compute $ F(x_{k},x_{k+1}) $
# $ \displaystyle F(a,b) = \sum_{k} F(x_{k},x_{k+1}) $
_ballblue_``Embarassingly parallel'' problem,\\ high speedup is expected._
[columns]>

==== A sketch of the parallel code ====
<[code][language=c++,basicstyle=\tiny\ttfamily,basicwidth=0.3em]
  MPI_Init(&argc, &argv);

  int rank, nprocs;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);

  // ...Get total number of steps, broadcast it...
  // ...

  // Each rank figures out its integration limits and number of steps
  unsigned long my_stepbase, my_nsteps;
  get_steps(nsteps_all, nprocs, rank,  &my_stepbase, &my_nsteps);
  
  const double per_step=(global_b-global_a)/nsteps_all;
  const double x1=global_a + my_stepbase*per_step;
  const double x2=x1 + my_nsteps*per_step;

  // Compute my own part of the integral
  double my_y=integral(integrand, my_nsteps, x1, x2);

  // Sum all numbers on master
  double y=0;
  MPI_Reduce(&my_y, &y, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);

  // ... print results ...
  // ...

  MPI_Barrier(MPI_COMM_WORLD);
  // Here we could start another computation.
  MPI_Finalize();
[code]>
==== Parallel performance: how to measure. ====
Now let's see how much we achieved...

* _blue_Strong scaling_: as we add processes, how do we fare?
* _blue_Weak scaling_: as we add ''both'' processes and work?

<[code][language=sh,emph={time}]
$ mpicc -O3 -o integral_par.x integral_par.cxx \
        -L./mylib -lmymath
$ time -p mpirun -np #(\textcolor{blue}{1}) ./integral_par.x 8000000
Result=3.775062 Exact=3.775063 Difference=-0.000001
real #(\textcolor{red}{17.23})
user #(\textcolor{purple}{17.08})
sys 0.02
$ time -p mpirun -np #(\textcolor{blue}{2}) ./integral_par.x 8000000
Result=3.775062 Exact=3.775063 Difference=-0.000001
real #(\textcolor{red}{17.24})
user #(\textcolor{purple}{31.98})
sys 0.05
[code]>

==== Parallel performance: results ====
\centering{'''Is there a performance problem?'''}
-- \baselineskip --
<[columns]
[[[0.5\textwidth]]]
<<<par_scaling.pdf,height=0.7\textheight>>>
[[[0.5\textwidth]]]
* Relative speedup: $s(p)=\frac{(\mathrm{time\ with}\ 1\ \mathrm{process})}{(\mathrm{time\ with}\ p\ \mathrm{processes})}$
* Ideal relative speedup: $s_{\mathrm{ideal}}(p) = p$
* Our speedup is \\!25\% on 12 nodes!\!
* I'd call it {``}''dismal''{\,''}.\\We '''do''' have a problem!
* Why? How to figure it out?
[columns]>

==== How does performance analysis work? ====
<[columns][T]
[[[0.6\textwidth]]]
_ballblue_How to collect data?_
* _blue_Instrumentation:_
** Insert timers \& counters\\ in the code
** Requires source or binary processing
* _blue_Sampling:_
** Interrupt \& check the program\\ at regular intervals
** Introduces statistical error
[[[0.55\textwidth]]]
_ballblue_What kind of data?_
* _blue_Profile:_
** Summary information only
** Relatively small file
* _blue_Trace:_
** Detailed recording\\during the run
** Potentially huge file
** Profile can be restored
[columns]>
-- 2\baselineskip --
<[center]
''Allinea MAP'' does _blue_tracing_ by _blue_sampling_.
[center]>


==== Prepare for profiling ====
To prepare for profiling/tracing, one needs to:
* Compile with full optimization
* Generate debugging symbols
* Link with system libs dynamically
** Usually the default
** Notable exception: Cray
* On Flux: load @ddt@ module
-- 1.5\baselineskip --
<[code][language=sh,emph={g,ddt}]
$ mpicc -g -O3 -o integral_par.x \
        integral_par.cxx -L ./mylib -lmymath
$ module add ddt
[code]>

==== Running Map: simple way (demo) ====
# Get interactive access to a compute node
# Change to your working directory
# Optionally, set ''sampling interval''
# Run as you would, prefixed by @_blue_map_@

<[code][language=sh,emph={I,X,map,ALLINEA_SAMPLER_INTERVAL}]
$ qsub -V -I -X -q flux -l qos=flux,nproc=12 \
        -l walltime=10:0:0 -A #(\emph{account\_flux})
$ cd $PBS_O_WORKDIR
$ export ALLINEA_SAMPLER_INTERVAL=5
$ map mpirun -np 12 ./integral_par.x 10000
[code]>

_otherblue_Caution_:
* Too small interval: large overhead!
* Too large interval: not enough samples!
* ''Allinea'' recommends at least 1000 samples/process

==== Running Map: other options ====
What if you can not or would not run a GUI? 
* Have slow or non-existing X connection to compute nodes.
* Do not want to wait for interactive session.
Use @_blue_-profile_@ option.
<[code][language=sh,emph={map,profile}]
#PBS -V
#PBS -q flux -l qos=flux -A #(\emph{account\_flux})
#PBS -l nproc=12,walltime=10:0:0
cd $PBS_O_WORKDIR
export ALLINEA_SAMPLER_INTERVAL=5
map -profile mpirun -np 12 ./integral_par.x 8000000
[code]>
This will create a  @_blue_*.map_@ file. 
Then run from the login node:
<[code][language=sh,emph={map}]
$ ma#()p integral_par_even_12p_*.map
[code]>

==== If you are submitting to a Flux queue... ====
<[columns][T]
[[[0.52\textwidth]]]
# Run @map@ from the login node:\\ \hspace{-1.8em}{\small @\$\ map ./integral\_par.x 8000000@}
# Set number of processes
# Check '''Submit to queue'''
# Click '''Configure...'''
# Load a proper ''submission template file'' (see next page)
# Click '''OK'''
# Click '''Run'''
[[[0.49\textwidth]]]
<<<allinea-map-run-menu.png,width=1.15\textwidth>>>
[columns]>

==== Submission template for Flux ====
<[code][language=sh]
#PBS -V
#PBS -l walltime=WALL_CLOCK_LIMIT_TAG
#PBS -l nodes=NUM_NODES_TAG:ppn=PROCS_PER_NODE_TAG
#PBS -q QUEUE_TAG -l qos=flux -A #(\emph{account\_flux})
#PBS -o PROGRAM_TAG-allinea.stdout
#PBS -e PROGRAM_TAG-allinea.stderr

cd $PBS_O_WORKDIR
AUTO_LAUNCH_TAG
[code]>


==== Time for a live demo!!! ====

<[columns]
[[[0.56\textwidth]]]
* Most of the time is spent in @MPI@;
* As run progresses, ''even more'' time is spent in @MPI@;
* Problem: some processes spend more time computing $f(x)$\\ then others!
* It's called ``_otherblue_Load Imbalance_''.
[[[0.6\textwidth]]]
<<<Allinea_map_par-12p.png,width=1\textwidth>>>
[columns]>
-- 0.5\baselineskip --
Possible solutions:

* Distribute work unevenly (but how?)
* Implement ''_ballblue_Dynamic Load Balancing_''.

==== Dynamic load balancing ====
_ballblue_Idea:_ If a process has nothing to do, make it to do something.
-- 0.5\baselineskip --
\centering{'''Manager-Workers approach:'''}
-- 0.2\baselineskip --
<[columns][T]
[[[0.5\textwidth]]]
\centering{_ballblue_Manager_}
# Listen to all workers
# Worker sent @_blue_READY_@ ?
#* send @_blue_GO_@ with a job chunk
# Worker sent @_blue_DONE_@?
#* add result to the sum
# No more job chunks?
#* send @_blue_STOP_@ to the worker
# No more workers?
#* we are done!
#* Otherwise, go to (1)
[[[0.5\textwidth]]]
\centering{_ballblue_Worker_}
# Send @_blue_READY_@ to the Manager
# Listen to the Manager
# Manager sent @_blue_GO_@ ?
#* Get job chunk
#* Do the calculation
#* Send @_blue_DONE_@ with result\\ to the Manager
#* Go to (1)
# Manager sent @_blue_STOP_@?
#* exit.
[columns]>

==== Dynamic Load Balancing: Worker code ====
<[code][language=c++,basicstyle=\tiny\ttfamily,basicwidth=0.3em]
void do_worker(int manager, int rank)
{
  for (;;) {
    // Inform manager that i am ready:
    MPI_Send(NULL,0, MPI_DOUBLE, manager, TAG_READY, MPI_COMM_WORLD);
    // ...and wait for the task.
    double range_data\[3\]; // expect x1, x2 and the number of steps (as double!) 
    MPI_Status stat;
    MPI_Recv(range_data,3,MPI_DOUBLE, manager, MPI_ANY_TAG, MPI_COMM_WORLD, &stat);
    switch (stat.MPI_TAG) {
    case TAG_GO:
      break; // do normal work
    case TAG_STOP:
      return;
    default:
      fprintf(stderr,"Rank %d: Got unexpected tag=%d, aborting.\n",rank,stat.MPI_TAG);
      MPI_Abort(MPI_COMM_WORLD, 2);
    }
    // we are here on TAG_GO.
    const double x1=range_data\[0\];
    const double x2=range_data\[1\];
    const unsigned long my_nsteps=range_data\[2\];
    // Compute my own part of the integral
    double my_y=integral(integrand, my_nsteps, x1, x2);

    // Send the result to manager
    MPI_Send(&my_y,1,MPI_DOUBLE, manager, TAG_DONE, MPI_COMM_WORLD);
  }
}
[code]> 

==== Dynamic Load Balancing: Manager code ====
<[code][language=c++,basicstyle=\tiny\ttfamily,basicwidth=0.3em]
double do_manager(const double global_a, const double global_b,
                 const unsigned long nsteps_all, const unsigned long points_per_block,
                 const int nprocs, const int rank)
{
  const double per_step=(global_b-global_a)/nsteps_all;
  
  int nworkers_left=nprocs-1;
  unsigned long ipoint=0; // next point to be processed
  double y=0;
  for (;;) {
    // Get a tagged message and possibly a result from any worker
    double y_worker=0;
    MPI_Status stat;
    MPI_Recv(&y_worker,1,MPI_DOUBLE, MPI_ANY_SOURCE,MPI_ANY_TAG, MPI_COMM_WORLD, &stat);
    const int rank_worker=stat.MPI_SOURCE;
    switch (stat.MPI_TAG) {
    case TAG_READY:
      // Do we have any work for this worker?
      if (ipoint>=nsteps_all) {
        // if not, stop the worker
        MPI_Send(NULL,0,MPI_DOUBLE, rank_worker, TAG_STOP, MPI_COMM_WORLD);
        --nworkers_left;
        break;
      }
[code]>

==== Dynamic Load Balancing: Manager code (cont.)  ====
<[code][language=c++,basicstyle=\tiny\ttfamily,basicwidth=0.3em]
      { // Prepare chunk of work for the worker
        const unsigned long ns_worker= 
           (ipoint+points_per_block > nsteps_all)? (nsteps_all - ipoint) : points_per_block;
        const double x1=global_a+ipoint*per_step;
        const double x2=x1+ns_worker*per_step;
        double range_data\[3\];
        range_data\[0\]=x1;
        range_data\[1\]=x2;
        range_data\[2\]=ns_worker;
        // Send the chunk
        MPI_Send(range_data,3,MPI_DOUBLE, rank_worker, TAG_GO, MPI_COMM_WORLD);
        // adjust the amount of points
        ipoint += ns_worker;
      }
      break;
    case TAG_DONE:
      y += y_worker;
      break;
    default:
      fprintf(stderr,"Rank %d (manager): Got unexpected tag=%d from %d,"
                     " aborting.\n",rank,rank_worker,stat.MPI_TAG);
      MPI_Abort(MPI_COMM_WORLD,1);
    }
    if (nworkers_left<=0) { // Any active workers left?
      return y;
    }
  }
}
[code]>

==== Dynamic Load Balancing: main  ====
<[code][language=c++,basicstyle=\tiny\ttfamily,basicwidth=0.3em]
int main(int argc, char** argv)
{
  MPI_Init(&argc, &argv);

  int rank, nprocs;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);

  // Get command line arguments, broadcast
  unsigned long int nsteps_all, points_per_block;
  // ....

  // Global integration limits.
  const double global_a=1E-5;
  const double global_b=1;

  // Split into workers and manager:
  if (rank==0) { // Run as the manager and get the result:
    double y=do_manager(global_a,global_b,nsteps_all,points_per_block,nprocs,rank);

    const double y_exact=4*(pow(global_b,0.25)-pow(global_a,0.25));
    printf("Result=%lf Exact=%lf Difference=%lf\n", y, y_exact, y-y_exact);
  } else { // Run as a worker 
    do_worker(0, rank);
  }

  MPI_Barrier(MPI_COMM_WORLD);
  // Here we could start another computation.
  MPI_Finalize();
  return 0;
}
[code]>

==== Dynamic Load Balancing: large block size (2500) ====
Run with:\\@\$ map mpirun -np 12 ./dyn\_integral.x 8000000 !2500! @
-- \baselineskip --
<[columns][T]
[[[0.68\textwidth]]]
<<<allinea_map_dyn_block2500.png,height=0.65\textheight>>>
[[[0.47\textwidth]]]
* CPU occupied till half-way
* Spikes in MPI use: Manager receiving data
* Looks like the last worker was holding everyone
* Other workers:\\''_babyblue_worker starvation_''\\(no work to do)
[columns]>

==== Dynamic Load Balancing: small block size (2) ====
Run with:\\@\$ map mpirun -np 12 ./dyn\_integral.x 8000000 !2! @
-- \baselineskip --
<[columns][T]
[[[0.65\textwidth]]]
<<<allinea_map_dyn_block2.png,height=0.65\textheight>>>
[[[0.47\textwidth]]]
* Brief useful CPU work, \\ then all time is in @MPI@
* Just moving data around:
** Workers receiving data
** 8\% (!1/12!) of time: Manager sending data.
* Very low ''_blue_computation/\\communication ratio_''.
[columns]>

==== Dynamic Load Balancing: good block size (100) ====
Run with:\\@\$ map mpirun -np 12 ./dyn\_integral.x 8000000 !100! @
-- \baselineskip --
<[columns][T]
[[[0.65\textwidth]]]
<<<allinea_map_dyn_block100.png,height=0.62\textheight>>>
[[[0.5\textwidth]]]
* _blue_80\%_ time CPU is busy!
* 8\% (!1/12!) of time: Manager work.
* Mostly @MPI@ by the\\ end of the run.
* OK computation/\\communication ratio.
* Still room for improvement!
[columns]>

==== Dynamic Load Balancing: Strong scaling graph. ====
\centering{'''How does it look now?'''}
<[columns]
[[[0.6\textwidth]]]
<<<dyn_scaling.pdf,height=0.8\textheight>>>
[[[0.55\textwidth]]]
Conclusions:
* Block size does affect performance.
* Block size 100 grows\\ up to node size (12).
* Too small block:\\ @MPI@ communication overhead.
* Too large block:\\ workers starvation.
[columns]>

==== Concluding slide ====
-- -1\baselineskip --
\centering{'''Take-home message'''}
\small
* Once you made your program parallel,\\ do simple scaling experiments.
* If scaling is bad, use profiling tools to understand why.
* ''Allinea Map'' is available for all Flux users.
* ''Map'' can analyze not only parallel, but single-node\\ and @OpenMP@ performance. (Can show something if time permits!)
* If you need any advise and/or help with your parallel programming,\\ARC provides _blue_free consulting service_
** Just send a mail to HPC support...
** ...or directly to: @"Alexander Gaenko"@ \url{<galexv@umich.edu>}
-- 0.5\baselineskip --
\normalsize\centering{''Thank you for your attention!''}

