<[autotemplate]
title={MPI profiling with Allinea MAP}
author={Alexander Gaenko}
usetheme={Frankfurt}
usecolortheme={whale}
lstloadlanguages={sh,C++}
definecolor={babyblue}{rgb}{0.54, 0.81, 0.94}
definecolor={ballblue}{rgb}{0.13, 0.67, 0.8}
definecolor={otherblue}{RGB}{120,100,255}
lstset={escapeinside={\#(}{)},style=basic,emphstyle=\color{otherblue},basicstyle=\small\ttfamily,basewidth=0.5em}
[autotemplate]>

==== Problem: Calculation of a definite integral.  ====

<[columns]
[[[0.25\textwidth]]]
<<<integrand-labelled.pdf,height=0.9\textheight>>>
[[[0.75\textwidth]]]
The problem:
* We need to compute: \\ $ \displaystyle F(a,b) = \int_a^b f(x) dx $ \\ Where $ f(x) $ is some (presumably\\ ``slow to compute'') function.\\[2\baselineskip]
* We use this approach:
*# Split $[a,b]$ by points {$\{x_{1}, x_{2}, \ldots, x_{k}\}$}
*# $ \displaystyle F(a,b) = \sum_{k} f(\frac{x_{k}+x_{k+1}}{2}) (x_{k+1}-x_{k}) $

[columns]>


==== Problem size behavior: how to measure. ====
Do we even need to parallelize? 
<[code][language=sh,emph={time}]
# Compilation:
$ gcc -O3 -o integral_seq.x integral_seq.cxx\
      -L./mylib -lmymath
# Timed runs:
$ time -p ./integral_seq.x #(\textcolor{blue}{1000000})
Result=3.775045 Exact=3.775063 Difference=-0.000019
real #(\textcolor{blue}{2.12})
$ time -p ./integral_seq.x #(\textcolor{blue}{2000000})
Result=3.775058 Exact=3.775063 Difference=-0.000005
real #(\textcolor{blue}{4.28})
$ time -p ./integral_seq.x #(\textcolor{blue}{8000000})
Result=3.775062 Exact=3.775063 Difference=-0.000001
real #(\textcolor{blue}{17.61})
[code]>
-- -0.5\baselineskip --
The time grows linearly with the problems size.
Acceptable accuracy at 8M points.
Can we speed it up?

==== Parallellization: domain decomposition. ====

Approach: 
* Split $[a,b]$ into several domains;
* Compute integrals independently.
<[columns]
[[[0.4\textwidth]]]
<<<integrand-edited.pdf,height=0.5\textheight>>>
[[[0.6\textwidth]]]
\hspace{2em}$ \displaystyle F(a,b) = \int_a^b f(x) dx $
# Assign a process to each \\ domain $[x_{k}, x_{k+1}]$
# Let each process \\ compute $ F(x_{k},x_{k+1}) $
# $ \displaystyle F(a,b) = \sum_{k} F(x_{k},x_{k+1}) $
[columns]>

==== A sketch of the parallel code ====
<[code][language=c++,basicstyle=\tiny\ttfamily,basicwidth=0.3em]
  MPI_Init(&argc, &argv);

  int rank, nprocs;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);

  // ...Get total number of steps, broadcast...

  // Each rank figures out its integration limits and number of steps
  unsigned long my_stepbase, my_nsteps;
  get_steps(nsteps_all, nprocs, rank,  &my_stepbase, &my_nsteps);
  
  const double per_step=(global_b-global_a)/nsteps_all;
  const double x1=global_a + my_stepbase*per_step;
  const double x2=x1 + my_nsteps*per_step;

  // Compute my own part of the integral
  double my_y=integral(integrand, my_nsteps, x1, x2);

  // Sum all numbers on master
  double y=0;
  MPI_Reduce(&my_y, &y, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);

  // ... print results ...
  MPI_Barrier(MPI_COMM_WORLD);
  // Here we could start another computation.
  MPI_Finalize();
[code]>
==== Parallel performance: how to measure. ====
Now let's see how much we achieved...

* _blue_Strong scaling_: as we add processes, how do we fare?
* _blue_Weak scaling_: as we add ''both'' processes and work?

<[code][language=sh,emph={time}]
$ mpicc -O3 -o integral_par.x integral_par.cxx \
        -L./mylib -lmymath
$ time -p mpirun -np 1 ./integral_par.x 8000000
Result=3.775062 Exact=3.775063 Difference=-0.000001
real #(\textcolor{red}{17.23})
user #(\textcolor{purple}{17.08})
sys 0.02
$ time -p mpirun -np 2 ./integral_par.x 8000000
Result=3.775062 Exact=3.775063 Difference=-0.000001
real #(\textcolor{red}{17.24})
user #(\textcolor{purple}{31.98})
sys 0.05
[code]>

==== Parallel performance: results ====
Is there a performance problem? 

!THIS GRAPH IS WRONG!
<<<integrand-labelled.pdf,height=0.7\textheight>>>

==== How does performance analysis work? ====
<[columns][T]
[[[0.6\textwidth]]]
_ballblue_How to collect data?_
* _blue_Instrumentation:_
** Insert timers \& counters\\ in the code
** Requires source or binary processing
* _blue_Sampling:_
** Interrupt \& check the program\\ at regular intervals
** Introduces statistical error
[[[0.55\textwidth]]]
_ballblue_What kind of data?_
* _blue_Profile:_
** Summary information only
** Relatively small file
* _blue_Trace:_
** Detailed recording\\during the run
** Potentially huge file
** Profile can be restored
[columns]>
-- 2\baselineskip --
<[center]
''Allinea MAP'' does _blue_tracing_ by _blue_sampling_.
[center]>


==== Prepare for profiling ====
To prepare for profiling, one needs:
* Compile with full optimization
* Generate debugging symbols
* Link with system libs dynamically
** Usually the default
** Notable exception: Cray
* On Flux: load @ddt@ module
-- 1.5\baselineskip --
<[code][language=sh,emph={g,ddt}]
$ mpicc -g -O3 -o integral_par.x \
        integral_par.cxx -L ./mylib -lmymath
$ module add ddt
[code]>

==== Running Map: simple way (demo) ====
# Get interactive access to a compute node
# Change to your working directory
# Optionally, set ''sampling interval''
# Run as you would, prefixed by @_blue_map_@

<[code][language=sh,emph={I,X,map,ALLINEA_SAMPLER_INTERVAL}]
$ qsub -V -I -X -q flux -l qos=flux,nproc=12 \
        -l walltime=10:0:0 -A #(\emph{account\_flux})
$ cd $PBS_O_WORKDIR
$ export ALLINEA_SAMPLER_INTERVAL=5
$ map mpirun -np 12 ./integral_par.x 10000
[code]>

_otherblue_Caution_:
* Too small interval: large overhead!
* Too large interval: not enough samples!

==== Running Map: other options ====
What if you can not or would not run GUI? 
* Have slow or non-existing X connection to compute nodes
* Do not want to run interactively
Use @_blue_-profile_@ option.
<[code][language=sh,emph={map,profile}]
#PBS -V
#PBS -q flux -l qos=flux -A #(\emph{account\_flux})
#PBS -l nproc=12,walltime=10:0:0
cd $PBS_O_WORKDIR
export ALLINEA_SAMPLER_INTERVAL=5
map -profile mpirun -np 12 ./integral_par.x 10000
[code]>
This will create a  @_blue_*.map_@ file. 
Then run from the login node:
<[code][language=sh,emph={map}]
$ ma#()p integral_par_even_12p_*.map
[code]>

==== If you are submitting to a Flux queue... ====
<[columns][T]
[[[0.52\textwidth]]]
# Run @map@ from the login node:\\ \hspace{-1.8em}{\small @\$\ map ./integral\_par.x 8000000@}
# Set number of processes
# Check '''Submit to queue'''
# Click '''Configure...'''
# Load a proper ''submission template file'' (see next page)
# Click '''OK'''
# Click '''Run'''
[[[0.49\textwidth]]]
<<<allinea-map-run-menu.png,width=1.15\textwidth>>>
[columns]>

==== Submission template for Flux ====
<[code][language=sh]
#PBS -V
#PBS -l walltime=WALL_CLOCK_LIMIT_TAG
#PBS -l nodes=NUM_NODES_TAG:ppn=PROCS_PER_NODE_TAG
#PBS -q QUEUE_TAG -l qos=flux -A #(\emph{account\_flux})
#PBS -o PROGRAM_TAG-allinea.stdout
#PBS -e PROGRAM_TAG-allinea.stderr

cd $PBS_O_WORKDIR
AUTO_LAUNCH_TAG
[code]>


==== Time for a live demo!!! ====

!SCREENSHOT HERE!

* Most of the time is spent in @MPI@
* As the run progresses, ''even more'' time is spent in @MPI@
* Problem: some processes spend more time \\ in calculating the integrand $f(x)$!
* It's called ``_otherblue_Load Imbalance_''

Possible solutions:

* Distribute work unevenly (but how?)
* Implement dynamic load balancing

==== Dynamic load balancing ====
\centering{Manager-Workers approach:}
-- 1\baselineskip --
<[columns][T]
[[[0.5\textwidth]]]
\centering{Manager}
# Listen to all workers
# Worker sent @READY@ ?\\ send @GO@ with a job chunk
# Worker sent @DONE@? \\ add result to the sum
# No more job chunks? \\ send @STOP@ to the worker
# No more workers? \\ we are done
[[[0.5\textwidth]]]
\centering{Worker}
# Send @READY@ to the Manager
# Listen to the Manager
# Manager sent @GO@ ?
#* Get job chunk
#* Do the calculation
#* Send @DONE@ with result\\ to the Manager
#* Go to (1)
# Manager sent @STOP@? \\ exit.
[columns]>

==== Strong scaling graph. ====
==== Map demo and screenshot. ====

==== The problem and the first approach ====

$$ I(a,b) = \int_a^b x^{-3/4} dx $$

Analytic solution to cross-check: $ I(a,b) = 4(b^{1/4} - a^{1/4}) $

<[columns]
[[[0.5\textwidth]]]
<<<integrand-edited.pdf,height=0.5\textheight>>>
[[[0.5\textwidth]]]
* Split $[a,b]$ equally between workers
* { $ I(a,b) = \sum_{k} I(x_{k},x_{k+1}) $ }
*<2> Caveat: integrand takes more time to compute at low $x$\!
[columns]>




==== Dynamic load balancing ====
Try various chunk sizes, various number of processes. \\
Where are the bottlenecks?


==== (Alternative: give smaller chunks to low X. Scalability. Profile.) ====

